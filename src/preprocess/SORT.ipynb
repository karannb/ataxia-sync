{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracking the patient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQllbXbZPcXI"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "*   If you have not run the `FRCNN.ipynb` notebook, that's the first step in the preprocessing pipeline, please run that before.\n",
        "*   This setup is just imports and result extraction from the previous stage.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prOPLB6oPcXK"
      },
      "source": [
        "### CoLab setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGMch2iiPcXL",
        "outputId": "d3630629-4809-4d9b-a638-75e99783e608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "ROOT = '/content/drive'\n",
        "from google.colab import drive\n",
        "drive.mount(ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_QQRPdEPcXM"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRi-TtgrPcXM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import shutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "SORT_PATH = \"/content/sort/\"\n",
        "FINAL_PATH = \"/content/final/\"\n",
        "JSONS_PATH = \"/content/jsons/\"\n",
        "VIDEO_PATH = \"/content/videos/\"\n",
        "FRAMES_PATH = \"/content/frames/\"\n",
        "RAW_VIDEOS_PATH = \"/content/drive/MyDrive/ataxia_dataset/\"\n",
        "# use CUDA if available\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqIZXUCiPcXN"
      },
      "source": [
        "#### Extract output from previous phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uPCR7M8oPcXN",
        "outputId": "3d40ab74-1666-4b8e-9ec6-a8e0565cc32f"
      },
      "outputs": [],
      "source": [
        "# extract frames\n",
        "shutil.copy(RAW_VIDEOS_PATH + \"frames.zip\", \"/content/\")\n",
        "!unzip frames.zip\n",
        "!mv content/frames/ /content/\n",
        "\n",
        "# extract jsons\n",
        "shutil.copy(RAW_VIDEOS_PATH + \"bboxes.zip\", \"/content/\")\n",
        "!unzip bboxes.zip\n",
        "!mv content/jsons/ /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4_GRGX5PcXN"
      },
      "source": [
        "---\n",
        "## 3. Object ID Tracking with SORT\n",
        "*   Simple Online and Realtime Tracking (SORT) algorithm for object ID tracking\n",
        "*   Quite fast (can be run on a CPU runtime), takes about 20m.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fumj77EvPcXO",
        "outputId": "70e71b1f-f918-4a8b-919e-7b0a275414ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'sort'...\n",
            "remote: Enumerating objects: 208, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 208 (delta 2), reused 1 (delta 1), pack-reused 203 (from 2)\u001b[K\n",
            "Receiving objects: 100% (208/208), 1.20 MiB | 19.28 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "# Git clone: SORT Algorithm\n",
        "!git clone https://github.com/abewley/sort.git\n",
        "sys.path.append(SORT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zskz4_NuPcXO",
        "outputId": "b9e5f42a-77f4-4859-ebd3-01ae6cf1acf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting filterpy==1.4.5 (from -r requirements.txt (line 1))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-image==0.17.2 (from -r requirements.txt (line 2))\n",
            "  Downloading scikit-image-0.17.2.tar.gz (29.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "# download requirements for SORT\n",
        "!cd \"$SORT_PATH\"; pip install -r requirements.txt\n",
        "!cd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "habyBqkcPcXO",
        "outputId": "8b2c3f79-1346-411a-8bc8-25f2e5e5f15a"
      },
      "outputs": [],
      "source": [
        "# Optional: if error occurs, you might need to re-install scikit-image, imgaug and filterpy\n",
        "\n",
        "# !pip install filterpy\n",
        "# !pip uninstall scikit-image -y\n",
        "# !pip uninstall imgaug -y\n",
        "# !pip install imgaug\n",
        "# !pip install -U scikit-image\n",
        "\n",
        "import skimage\n",
        "print(skimage.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbBzLAxFPcXP"
      },
      "outputs": [],
      "source": [
        "!export MPLBACKEND=Agg # NOTE TKAgg doesn't work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7icsH1TPcXP"
      },
      "source": [
        "At this point, the next cell will give an error with TKAgg, you must modify the **23rd** line in `/content/sort/sort.py` from TKAgg to Agg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqQgnk1HPcXP",
        "outputId": "29e2190a-b66f-4669-d0d4-95dad242bae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For video 000, we have 180 frames\n"
          ]
        }
      ],
      "source": [
        "from sort import *\n",
        "\n",
        "example = join(JSONS_PATH, '000.json')\n",
        "with open(example) as data_file:\n",
        "   data = json.load(data_file)\n",
        "odata = collections.OrderedDict(sorted(data.items()))\n",
        "print(f\"For video 000, we have {len(odata)} frames\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bynv_5SPcXP"
      },
      "source": [
        "---\n",
        "\n",
        "*   We can't easily track each patient as is because of the blurring (this is also reported in https://github.com/ROC-HCI/Automated-Ataxia-Gait)\n",
        "*   In fact, we do not even need the exact bbox, once the first frame has a majority of the patient, OpenPose can track them\n",
        "*   We do not normalize the height like in the Auto-Gait paper\n",
        "*   Additionally, because of the same blurring the video is out-of-distribution for most models, so we keep the full height, else it becames too pixeled\n",
        "*   Finally, some images have no people detected by the FRCNN, we use the previous frame's predictions for this\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK5kYZuQPcXQ"
      },
      "outputs": [],
      "source": [
        "# Making new directory for saving results\n",
        "!mkdir \"$FINAL_PATH\"\n",
        "progress = [] # in case of errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd_rcfEKPcXQ",
        "outputId": "56dbb582-c4e5-41c5-ebfa-1b28229d6a11"
      },
      "outputs": [],
      "source": [
        "for vid_path in tqdm(sorted(os.listdir(FRAMES_PATH))):\n",
        "  # Create a tracker using SORT Algorithm\n",
        "  mot_tracker = Sort()\n",
        "  # vid is like xxx\n",
        "  cur_save_path = join(FINAL_PATH, vid_path)\n",
        "  # this becomes /content/final/xxx: we will save all tracked objects inside this\n",
        "  # folder for the xxx video in separate folders\n",
        "  if not os.path.exists(cur_save_path):\n",
        "    os.mkdir(cur_save_path)\n",
        "  elif vid_path in progress:\n",
        "    print(f\"Already processed {vid_path}, skipping...\")\n",
        "    continue\n",
        "  # load preds for current video\n",
        "  odata = json.load(open(join(JSONS_PATH, vid_path + \".json\")))\n",
        "  odata = collections.OrderedDict(sorted(odata.items()))\n",
        "  # book-keeping variables\n",
        "  heights = {}\n",
        "  IDs = set()\n",
        "  first_frame_widths = {}\n",
        "  result = None\n",
        "  # key = frame_num\n",
        "  for key in sorted(odata.keys()):\n",
        "    arrlist = []\n",
        "    # load the image\n",
        "    det_img = cv2.imread(os.path.join(FRAMES_PATH, vid_path, key))\n",
        "    # load the predictions for this image (bbox, labels and score)\n",
        "    tmp_res = odata[key]\n",
        "    if len(tmp_res) == 0:\n",
        "      print(f\"Empty prediction at frame {key} in video {vid_path}, setting to previous prediction.\")\n",
        "      # do not update the result variable\n",
        "    else:\n",
        "      result = tmp_res\n",
        "\n",
        "    # run the tracker update\n",
        "    for info in result:\n",
        "      bbox = info['bbox']\n",
        "      # labels = info['labels'], labels are useless for us, if this was a general purpose\n",
        "      # thing then maybe useful, but right now we have already filtered for humans\n",
        "      scores = info['scores']\n",
        "      # this is the format that MoT expects\n",
        "      templist = bbox+[scores]\n",
        "      arrlist.append(templist) # in the genral case, we would filter by labels here\n",
        "\n",
        "    # update the tracker with this new frame info.\n",
        "    track_bbs_ids = mot_tracker.update(np.array(arrlist))\n",
        "\n",
        "    for j in range(track_bbs_ids.shape[0]):\n",
        "      xy_xy_label = track_bbs_ids[j, :]\n",
        "      x = int(xy_xy_label[0])\n",
        "      x = max(x - 50, 0) # sometimes bounding boxes are too tight\n",
        "      y = int(xy_xy_label[1])\n",
        "      x2 = int(xy_xy_label[2])\n",
        "      x2 = min(x2 + 50, det_img.shape[1])\n",
        "      y2 = int(xy_xy_label[3])\n",
        "      track_label = str(int(xy_xy_label[4]))\n",
        "\n",
        "      # we also tried cropping according to ONLY the first frame but it was quite wide\n",
        "      # if \"001\" in str(key):\n",
        "      #   print(f\"{j}th valid person detected for {vid_path}.\")\n",
        "      #   # add the first frame bbox to the first_frame_widths dict\n",
        "      #   first_frame_widths[track_label] = (x, x2)\n",
        "      # elif track_label not in first_frame_widths:\n",
        "      #     print(f\"New person detected at frame {key} for {vid_path}, track_label: {track_label}.\")\n",
        "      #     continue\n",
        "\n",
        "      # get the height of the bbox\n",
        "      if track_label not in heights:\n",
        "        print(f\"New person detected at frame {key} for {vid_path}, track_label: {track_label}.\")\n",
        "        heights[track_label] = []\n",
        "      heights[track_label].append(y2 - y)\n",
        "\n",
        "      # crop each person with along the width according to their first bbox, but keep the full height\n",
        "      # cropped_img = det_img[:, first_frame_widths[track_label][0]:first_frame_widths[track_label][1]]\n",
        "      cropped_img = det_img[:, x:x2]\n",
        "      # make a directory for this 'track_label'\n",
        "      os.makedirs(f'{cur_save_path}/' + track_label, exist_ok=True)\n",
        "      if isinstance(cropped_img, np.ndarray):\n",
        "        try:\n",
        "          # save cropped video and landmark data in separate folders\n",
        "          cv2.imwrite(f'{cur_save_path}/' + track_label + '/person_' + track_label + '_' + key, cropped_img)\n",
        "        except Exception as e:\n",
        "          print(f\"vid_path: {vid_path}, key: {key}, track_label: {track_label}, error: {e}\")\n",
        "          continue\n",
        "\n",
        "  # now this video is processed, we can check which participant had maximum score\n",
        "  # this is fro the auto-gait paper itself\n",
        "  # score = sum of diffs in heights\n",
        "  max_score = -1\n",
        "  max_k = None\n",
        "  for k, v in heights.items():\n",
        "    if len(v) > 0:\n",
        "      scores = -np.diff(v) # negative because we want to pick the person with decreasing height\n",
        "      scores = np.sum(scores)\n",
        "      if scores > max_score:\n",
        "        max_score = scores\n",
        "        max_k = k\n",
        "    else:\n",
        "      print(f\"Empty height for {k} in {vid_path}, skipping...\")\n",
        "      continue\n",
        "  if max_k is not None:\n",
        "    # this is the participant with maximum score\n",
        "    print(f\"Max score for {vid_path} is {max_score}, for participant {max_k}\")\n",
        "    # delete frames for other participants\n",
        "    for k, v in heights.items():\n",
        "      if k != max_k:\n",
        "        shutil.rmtree(f'{cur_save_path}/' + str(k))\n",
        "        print(f\"Deleted {k} from {vid_path}\")\n",
        "  progress.append(vid_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZbfEIVPcXR"
      },
      "source": [
        "We have now a list of frames with largely only the patient in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip final_frames.zip -r \"$FINAL\"\n",
        "shutil.copy(\"final_frames.zip\", RAW_VIDEOS_PATH) # the frames will be in your drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3T-Gb3jPcXR"
      },
      "source": [
        "---\n",
        "## 4. Create videos and move to drive for manual inspection\n",
        "*   We will make videos for all the people tacked, following the same directory structure and then move the folder to our drive.\n",
        "*   Quite fast (can be run on a CPU runtime), takes about 15m.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6Zyb9ToPcXR"
      },
      "outputs": [],
      "source": [
        "# Make new directory for saving videos\n",
        "!mkdir \"$VIDEO_PATH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJWgbIW9PcXR",
        "outputId": "e351d85f-78f9-4f6b-92e9-c162db5cfd16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 149/149 [14:09<00:00,  5.70s/it]\n"
          ]
        }
      ],
      "source": [
        "for vid_path in tqdm(sorted(os.listdir(FINAL_PATH))):\n",
        "  cur_save_path = join(FINAL_PATH, vid_path)\n",
        "  if len(os.listdir(cur_save_path)) == 1:\n",
        "    track_label = os.listdir(cur_save_path)[0]\n",
        "    # directly save the video\n",
        "    video_maker = [\"ffmpeg\",\n",
        "                   \"-framerate\", \"30\",\n",
        "                   \"-pattern_type\", \"glob\",\n",
        "                   \"-i\", os.path.join(FINAL_PATH, vid_path, track_label, \"*.jpg\"),\n",
        "                   \"-c:v\", \"libx264\",\n",
        "                   \"-vf\", \"pad=ceil(iw/2)*2:ceil(ih/2)*2\",\n",
        "                   \"-pix_fmt\", \"yuv420p\",\n",
        "                   join(VIDEO_PATH, vid_path + \".mp4\")]\n",
        "    out = subprocess.run(video_maker, stderr=subprocess.PIPE)\n",
        "  else:\n",
        "    os.mkdir(join(VIDEO_PATH, vid_path))\n",
        "    for track_label in sorted(os.listdir(cur_save_path)):\n",
        "      video_maker = [\"ffmpeg\",\n",
        "                    \"-framerate\", \"30\",\n",
        "                    \"-pattern_type\", \"glob\",\n",
        "                    \"-i\", os.path.join(FINAL_PATH, vid_path, track_label, \"*.jpg\"),\n",
        "                    \"-c:v\", \"libx264\",\n",
        "                    \"-pix_fmt\", \"yuv420p\",\n",
        "                    join(VIDEO_PATH, vid_path, track_label + \".mp4\")]\n",
        "      out = subprocess.run(video_maker, stderr=subprocess.PIPE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nf0InKkjPcXS",
        "outputId": "07fea3e8-7c0a-44c7-8bb7-753e873ce7a5"
      },
      "outputs": [],
      "source": [
        "shutil.copytree(VIDEO_PATH, \"/content/drive/MyDrive/ataxia_dataset/patient_videos/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXUz3H7wPcXS"
      },
      "source": [
        "## Thank you!\n",
        "Now we can move on to further processing using these videos to extract keypoints with OpenPose.\n",
        "*   We have manually inspected these and ensured that all of them process sucessfully and have the patient in them for a majority of the time, especially at the start.\n",
        "*   However, after the OpenPose extraction you would need to investigate further manually.\n",
        "*   The videos might look funny, but the frames are properly created, as the height is the same across videos, when you play them they might start bulging or shrinking."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
