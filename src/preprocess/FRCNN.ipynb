{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting frames and bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup\n",
    "*   You need to mount your drive and create a folder named `ataxia_dataset/` with all the videos (in mp4 format).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoLab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/content/drive'\n",
    "from google.colab import drive\n",
    "drive.mount(ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "JSONS_PATH = \"/content/jsons/\"\n",
    "FRAMES_PATH = \"/content/frames/\"\n",
    "RAW_VIDEOS_PATH = \"/content/drive/MyDrive/ataxia_dataset/\"\n",
    "# use CUDA if available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Extract frames using `ffmpeg`\n",
    "*   We will extract frames from all videos at 30 fps and store them temporarily on CoLab.\n",
    "*   Takes about 17m.\n",
    "*   Advice: Run this on a CPU instance and follow the instructions in the end.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Directory\n",
    "!mkdir \"$FRAMES_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(fname: str):\n",
    "  \"\"\"\n",
    "  Extract frames from a video at 30 FPS, must be in `x.mp4` format.\n",
    "  Frames are saved in `raw_videos_path/video_num/output_xxx.jpg`.\n",
    "  \"\"\"\n",
    "  # ignore other files if present in the directory\n",
    "  if \".mp4\" not in fname:\n",
    "    print(f\"{fname} is not a video, skipping...\")\n",
    "    return None\n",
    "\n",
    "  # 1.mp4 -> 1 conversion + :03 to ensure sorting\n",
    "  video_num = f\"{int(fname.split('.mp4')[0]):03}\"\n",
    "  if os.path.exists(os.path.join(FRAMES_PATH, video_num)):\n",
    "      print(f\"Already processed {video_num:03}, skipping...\")\n",
    "      return None\n",
    "\n",
    "  # make the output directory for this video\n",
    "  os.mkdir(os.path.join(FRAMES_PATH, video_num))\n",
    "\n",
    "  # Break the video into frames at 30 FPS\n",
    "  command = [\"ffmpeg\",\n",
    "              \"-i\",\n",
    "              os.path.join(RAW_VIDEOS_PATH, fname), # e.g. raw_videos_path/num.mp4\n",
    "              \"-vf\",\n",
    "              \"fps=30\",\n",
    "              os.path.join(FRAMES_PATH, video_num, \"output_%03d.jpg\")] # e.g. /path/001/output_001.jpg\n",
    "  subprocess.run(command, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input for this function\n",
    "videos = os.listdir(RAW_VIDEOS_PATH)\n",
    "# CoLab allows 2 CPUs in the free tier\n",
    "with Pool(processes=2) as pool:\n",
    "  list(tqdm(pool.imap_unordered(process_video, videos), total=len(videos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Original picture before detection\n",
    "person0 = join(FRAMES_PATH, '000')\n",
    "person0_frames = os.listdir(person0)\n",
    "person0_frames.sort()\n",
    "\n",
    "print(join(person0, person0_frames[0]))\n",
    "img_ex_path = join(person0, person0_frames[0])\n",
    "img_ex_origin = cv2.imread(img_ex_path)\n",
    "img_ex = cv2.cvtColor(img_ex_origin, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_ex)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is advisable to download / save the extracted frames as a zip so as to not run this extraction again in case of any errors in the next stage (the file size is about 700MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip frames.zip -r \"$FRAMES_PATH\"\n",
    "shutil.copy(\"frames.zip\", RAW_VIDEOS_PATH) # the frames will be in your drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract processed frames\n",
    "If you followed the advice and ran it on a CPU only runtime, we will now need to switch to a GPU runtime for FRCNN, you can run the prvious cell to save the frames and then extract using the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment the lines below to unzip the frames extracted earlier\n",
    "# shutil.copy(RAW_VIDEOS_PATH + \"frames.zip\", \"/content/\")\n",
    "# !unzip frames.zip\n",
    "# !mv content/frames/ /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Object Detection with Faster R-CNN\n",
    "\n",
    "*  We will use a pretrained Faster R-CNN model using ResNet50 as a backbone with FPN.\n",
    "*  Takes about 1h20m.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the pretrained Faster R-CNN model from torchvision\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True).to(DEVICE) #cuda speeds up by ~8x\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class names given by PyTorch's official Docs\n",
    "\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Original picture before detection\n",
    "person0 = join(FRAMES_PATH, '000')\n",
    "person0_frames = os.listdir(person0)\n",
    "person0_frames.sort()\n",
    "\n",
    "print(join(person0, person0_frames[0]))\n",
    "img_ex_path = join(person0, person0_frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prediction\n",
    "threshold = 0.8\n",
    "# Load the image\n",
    "img = Image.open(img_ex_path)\n",
    "# Defing PyTorch Transform\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "# Apply the transform to the image\n",
    "img = transform(img).to(DEVICE)\n",
    "start = time.time()\n",
    "# Pass the image to the model\n",
    "pred = model([img])\n",
    "end = time.time()\n",
    "print(f\"Took {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pred` is a list of batched-predictions, if the model was input a batch-size of 2, it will be of length 2, and such. **Importantly** the model's predictions are always sorted in a **descending** order by score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch-size={len(pred)}.\")\n",
    "print(f\"An object is represented as a dictionary with keys: {pred[0].keys()}.\")\n",
    "print(f\"The model found: {len(pred[0]['boxes'])} predictions.\")\n",
    "print(f\"pred[0]['boxes']: {pred[0]['boxes']}\")\n",
    "print(f\"pred[0]['labels']: {pred[0]['labels']}\")\n",
    "print(f\"pred[0]['scores']: {pred[0]['scores']}\")\n",
    "\n",
    "# Get Prediction Labels for each prediction\n",
    "pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())]\n",
    "print(f\"Classes detected: {pred_class}\")\n",
    "# Appropriately parse Bounding boxes (outputted by model as [x1, y1, x2, y2])\n",
    "pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy().astype(np.int32))]\n",
    "pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
    "# Get the last index with score greater than threshold\n",
    "pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "pred_boxes = pred_boxes[:pred_t+1]\n",
    "pred_class = pred_class[:pred_t+1]\n",
    "print(f\"Classes w/ scores > threshold: {pred_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will display the bounding box overlayed on the image\n",
    "img = cv2.imread(img_ex_path) # Read image with cv2\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "for i in range(len(pred_boxes)):\n",
    "    cv2.rectangle(img, pred_boxes[i][0], pred_boxes[i][1], color=(0, 255, 0), thickness=3) # Draw Rectangle with the coordinates\n",
    "    cv2.putText(img,pred_class[i], pred_boxes[i][0],  cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0),thickness=3) # Write the prediction class\n",
    "    plt.figure(figsize=(15,20)) # display the output image\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus, Define a function for get a prediction result from the model\n",
    "# this can be batched for multiple images\n",
    "def get_prediction(img_path: str, threshold: float):\n",
    "  img = Image.open(img_path).convert('RGB') # Load the image\n",
    "  img = transform(img).to(DEVICE) # Apply the transform to the image\n",
    "  pred = model([img]) # Pass the image to the model\n",
    "\n",
    "  # Mask out to only keep people and confidence > threshold here itself\n",
    "  mask = (pred[0]['labels'] == 1) & (pred[0]['scores'] > threshold)\n",
    "  boxes = pred[0]['boxes'][mask]\n",
    "  labels = pred[0]['labels'][mask]\n",
    "  scores = pred[0]['scores'][mask]\n",
    "\n",
    "  # Get the Prediction Score\n",
    "  pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(labels.cpu().numpy())]\n",
    "  # Bounding boxes, the type conversion to np.int32 is necessary for cv2 later\n",
    "  pred_boxes = [(i[0], i[1], i[2], i[3]) for i in list(boxes.detach().cpu().numpy().astype(np.int32))]\n",
    "  pred_score = list(scores.detach().cpu().numpy())\n",
    "\n",
    "  return pred_boxes, pred_class, pred_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   The picture above is an example of applying Detection Network (in our case, Faster R-CNN).\n",
    "*   Since the purpose of dataset we are using is 'tracking', we only keep the 'person' class in our predictions.\n",
    "*   We need a prediction result (bounding box offset, class label, pred scores) for all the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Directory\n",
    "!mkdir \"$JSONS_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bboxes, labels and scores and store as a json in proper format\n",
    "# Data Structure format from - https://github.com/mlvlab/COSE474/blob/master/3_MOT_detinfo.json\n",
    "for video in tqdm(sorted(os.listdir(FRAMES_PATH))):\n",
    "  person_data = join(FRAMES_PATH, video)\n",
    "  person_json = {}\n",
    "\n",
    "  data_list = os.listdir(person_data)\n",
    "  data_list.sort()\n",
    "\n",
    "  for frame in data_list:\n",
    "    cur_frame = join(FRAMES_PATH, video, frame)\n",
    "    pred_boxes, pred_class, pred_score = get_prediction(cur_frame, 0.9)\n",
    "    data_list = [{\"bbox\": [int(bbox[i]) for i in range(4)],\n",
    "                  \"labels\": 1, # label will always be 1 as we are only detecting people.\n",
    "                  \"scores\": float(score)} for bbox, score in zip(pred_boxes, pred_score)]\n",
    "    person_json[frame] = data_list\n",
    "\n",
    "  f = open(join(JSONS_PATH, video + \".json\"), \"w\")\n",
    "  json.dump(person_json, f, indent=4)\n",
    "  f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move the result to the head directory (you can download as well for safe-keeping, this file is quite small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r bboxes.zip /content/jsons\n",
    "shutil.copy(\"bboxes.zip\", RAW_VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!\n",
    "No we can move on to further processing using these frames and jsons to actually track the patient in the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra! Make a video with the bounding boxes (I used this for the tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment the lines below to unzip the bboxes extracted earlier\n",
    "# shutil.copy(RAW_VIDEOS_PATH + \"bboxes.zip\", \"/content/\")\n",
    "# !unzip bboxes.zip\n",
    "# !mv content/jsons/ /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_VIDS_PATH = \"/content/box_vids/\"\n",
    "!mkdir \"$BOX_VIDS_PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in tqdm(sorted(os.listdir(FRAMES_PATH))):\n",
    "  person_data = join(FRAMES_PATH, video)\n",
    "  person_json = json.load(open(join(JSONS_PATH, video + \".json\")))\n",
    "\n",
    "  data_list = os.listdir(person_data)\n",
    "  data_list.sort()\n",
    "\n",
    "  for frame in data_list:\n",
    "    pred_boxes, pred_class, pred_score = person_json[frame], person_json[frame], person_json[frame]\n",
    "    frame_path = join(person_data, frame)\n",
    "    # This will display the bounding box overlayed on the image\n",
    "    img = cv2.imread(frame_path) # Read image with cv2\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "    for i in range(len(pred_boxes)):\n",
    "        pt1 = (pred_boxes[i]['bbox'][0], pred_boxes[i]['bbox'][1])\n",
    "        pt2 = (pred_boxes[i]['bbox'][2], pred_boxes[i]['bbox'][3])\n",
    "        cv2.rectangle(img, pt1, pt2, color=(0, 255, 0), thickness=3) # Draw Rectangle with the coordinates\n",
    "        # save to BOX_VIDS / video / frame.png\n",
    "        if not os.path.exists(join(BOX_VIDS_PATH, video)):\n",
    "          os.mkdir(join(BOX_VIDS_PATH, video))\n",
    "        cv2.imwrite(join(BOX_VIDS_PATH, video, frame), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a video with ffmpeg\n",
    "!ffmpeg -framerate 30 -i /content/box_vids/001/output_%03d.jpg -c:v libx264 -pix_fmt yuv420p output_video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
